{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scraping part 1: Oscars winners and nominee(1990-2019)\n",
    "\n",
    "Here, we make use of Selenium to help us get on the Oscars website and scrape all the info regarding Oscar-winning and Oscar nominated films from **1990** to **2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "PATH = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "chromedriver=webdriver.Chrome(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver.get(\"http://google.com\")\n",
    "time.sleep(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_button1 = chromedriver.find_element_by_xpath(\n",
    "    '//button[contains(@class, \"yearsfrom\")]'\n",
    ")\n",
    "\n",
    "year_button1.click()\n",
    "\n",
    "year_button2=chromedriver.find_element_by_xpath(\n",
    "    '//*[@id=\"basicsearch\"]/div/div[2]/div[2]/div/div[1]/span/div/ul/li[32]/a/label/input'\n",
    "    )\n",
    "\n",
    "year_button2.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_button3=chromedriver.find_element_by_xpath(\n",
    "    '//*[@id=\"basicsearch\"]/div/div[2]/div[2]/div/div[2]/span/div/button'\n",
    ")\n",
    "\n",
    "year_button3.click()\n",
    "\n",
    "\n",
    "year_button4=chromedriver.find_element_by_xpath(\n",
    "    '//*[@id=\"basicsearch\"]/div/div[2]/div[2]/div/div[2]/span/div/ul/li[3]/a/label/input'\n",
    ")\n",
    "\n",
    "year_button4.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=chromedriver.find_element_by_xpath(\n",
    "    '//*[@id=\"btnbasicsearch\"]'\n",
    ")\n",
    "\n",
    "search_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(chromedriver.page_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we make a web scraping function that will get all the relevent data needed and put them in a list. Those include:award category, year of the release, movie name, nominated individuals, and whether or not it ended up winning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cateogries_by_year(soup,full_list):\n",
    "    for group in soup.find_all('div', class_='awards-result-chron'):\n",
    "        for subgroup in group.find_all('div',class_='result-subgroup'):\n",
    "                for details in subgroup.find_all('div',class_='result-details'):\n",
    "                    full_list.append(subgroup.find('div',class_='result-subgroup-header').find('a').text)\n",
    "                    full_list.append(group.find('div',class_='result-group-header').text)\n",
    "                    full_list.append(details.text)\n",
    "                    full_list.append(str(bool(details.find('span',class_='glyphicon'))))\n",
    "    return full_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning part 1:\n",
    "Data scraped from the oscars website is cleaned and converted to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets first clean the scraped data by first creating a list and getting rid of some special characters, items in brackets, and distracting line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list=[]\n",
    "cateogries_by_year(soup,full_list);\n",
    "import re\n",
    "full_list=[i.replace('\\n','') for i in full_list]\n",
    "full_list=[re.sub('\\{.*\\}','',i) for i in full_list]\n",
    "full_list=[re.sub('\\s\\(\\d\\d\\w\\w\\)','',i) for i in full_list]\n",
    "full_list=[i.split('--') for i in full_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's make a dictionary of all the list items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict={}\n",
    "\n",
    "\n",
    "def lists_maker(full_list,catgory_dict):\n",
    "    c=0\n",
    "    i=0\n",
    "    while c < len(full_list) :\n",
    "        if len(full_list[c+2])==2:\n",
    "            category_dict[i]=[full_list[c][0]]+[full_list[c+1][0]]+[full_list[c+2][0]]+[full_list[c+2][1]]+[full_list[c+3][0]]\n",
    "            i+=1\n",
    "        c+=4\n",
    "        \n",
    "    return category_dict\n",
    "\n",
    "lists_maker(full_list,category_dict)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, let's name the columns appropriately and swap the wrongfully placed column values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "movies_df=pd.DataFrame(category_dict).T\n",
    "movies_df.rename(columns={0: 'category'}, inplace=True)\n",
    "movies_df.rename(columns={1: 'year'}, inplace=True)\n",
    "movies_df.rename(columns={2: 'movie'}, inplace=True)\n",
    "movies_df.rename(columns={3: 'names'}, inplace=True)\n",
    "movies_df.rename(columns={4: 'oscar_win'}, inplace=True)\n",
    "movies_df.loc[(movies_df['category']=='ACTOR IN A LEADING ROLE')|(movies_df['category']=='ACTOR IN A SUPPORTING ROLE')|(movies_df['category']=='ACTRESS IN A LEADING ROLE')|(movies_df['category']=='ACTRESS IN A SUPPORTING ROLE'),['movie','names']]=movies_df.loc[(movies_df['category']=='ACTOR IN A LEADING ROLE')|(movies_df['category']=='ACTOR IN A SUPPORTING ROLE')|(movies_df['category']=='ACTRESS IN A LEADING ROLE')|(movies_df['category']=='ACTRESS IN A SUPPORTING ROLE'),['names','movie']].values\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, let's clean the data more and get rid of empty spaces. We're also getting rid of several categories such as \n",
    "international films and documentaries as they are not the focus of the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_title_clean(movies_df):\n",
    "    for i in range(len(movies_df['movie'])):\n",
    "        movies_df['movie'][i]=re.sub('\".*\"\\w\\w\\w\\w','',movies_df['movie'][i])\n",
    "        movies_df['movie'][i].strip()\n",
    "        movies_df['names'][i].strip()\n",
    "        movies_df['movie'][i]=movies_df['movie'][i].lower()\n",
    "        if movies_df['oscar_win'][i]=='False':\n",
    "            movies_df['oscar_win'][i]=0\n",
    "        else:\n",
    "            movies_df['oscar_win'][i]=1\n",
    "    return movies_df\n",
    "movie_title_clean(movies_df)\n",
    "movies_df=movies_df.loc[(movies_df['category']!='INTERNATIONAL FEATURE FILM')&(movies_df['category']!='FOREIGN LANGUAGE FILM')&(movies_df['category']!='SCIENTIFIC AND TECHNICAL AWARD (Scientific and Engineering Award)')&(movies_df['category']!='DOCUMENTARY (Feature)')&(movies_df['category']!='DOCUMENTARY (Short Subject)')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many times each movie was nominated and how many oscars it won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 3500)\n",
    "movies_df['times_nominated']=movies_df.groupby('movie')['movie'].transform('count')\n",
    "new_df=movies_df.groupby(['movie','times_nominated'],as_index=False)[['oscar_win']].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now lets visualize what that looks like with a bar graph.Hmm,it seems like its a linear relationship for the most part: the more categories a movie is nominated for an oscar, the more oscar its going to win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x=new_df['times_nominated']\n",
    "plt.xlabel('times nominated')\n",
    "plt.ylabel('Oscars won')\n",
    "y=new_df['oscar_win']\n",
    "plt.bar(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection part 2: boxoffice mojo\n",
    "For this section, I am going to scrape box office mojo for the movies' release month(its important, more details to follow), budget, domestic boxoffice, MPAA rating, genres and runtime. This is done by creating a selenium loop and several helper functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first lets create some empty columns with the appropriate header to fill our values later\n",
    "new_df['domestic_boxoffice']=''\n",
    "new_df['distributor']=''\n",
    "new_df['budget']=''\n",
    "new_df['release_month']=''\n",
    "new_df['MPAA_rating']=''\n",
    "new_df['running_time']=''\n",
    "new_df['genre']=''\n",
    "new_df['tropes']=''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_movie_value(soup, field_name):\n",
    "    \n",
    "    '''Grab a value from Box Office Mojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and returns the string in\n",
    "    the next sibling object (the value for that attribute) or None if nothing is found.\n",
    "    '''\n",
    "    \n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    \n",
    "    if not obj: \n",
    "        return None\n",
    "    \n",
    "    # this works for most of the values\n",
    "    next_element = obj.findNext()\n",
    "    \n",
    "    if next_element:\n",
    "        return next_element.text \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(chromedriver.page_source)\n",
    "genre=get_movie_value(soup, 'Genres')\n",
    "companystr=get_movie_value(soup, 'Domestic Distributor')\n",
    "\n",
    "box=get_movie_value(soup, 'Budget')\n",
    "\n",
    "rating=get_movie_value(soup, 'MPAA')\n",
    "print(rating)\n",
    "\n",
    "\n",
    "# domestic box office:\n",
    "soup.find('span',class_='money').text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# parse the genre:\n",
    "def genre_list(genre):\n",
    "    try:\n",
    "        genre=re.sub('\\n\\s*',',',genre)\n",
    "        genre=genre.split(',')\n",
    "        return genre\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# parse the company:\n",
    "def production_company(companystr):\n",
    "    try:\n",
    "        company=companystr.split('See')[0]\n",
    "        return company\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# parse any money amount:\n",
    "def money_to_int(moneystring):\n",
    "    try:\n",
    "        moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "        return int(moneystring)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# parse any runtime:\n",
    "def runtime_to_minutes(runtimestring):\n",
    "    try:\n",
    "        runtime = runtimestring.split()\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# searching for boxoffice:\n",
    "def soup_find_money(soup):\n",
    "    try:\n",
    "        money=soup.find('span',class_='money').text\n",
    "        return money\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# parse date to its month: \n",
    "def to_date(datestr):\n",
    "    try:\n",
    "        date=datestr.split('/n')[0]\n",
    "        date = dt.strptime(datestr.split('\\n')[0],'%B %d, %Y')\n",
    "        return date.month\n",
    "    except:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a scrape bot that will search each of the movies in our list on boxofficemojo and return all the info we need \n",
    "#info is then stored in the dataframe.\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "def boxoffice_mojo_scrape_bot(new_df):\n",
    "    i=0\n",
    "    while i < len(new_df):\n",
    "        chromedriver.find_elements_by_xpath('//*[@id=\"mojo-search-text-input\"]')[0].send_keys(new_df['movie'][i])\n",
    "        chromedriver.find_elements_by_xpath('//*[@id=\"mojo-search-text-input\"]')[0].send_keys(Keys.ENTER)\n",
    "        soup = BeautifulSoup(chromedriver.page_source)\n",
    "        if bool('No results' not in soup.find('div','mojo-gutter').text):\n",
    "            chromedriver.find_elements_by_xpath('//*[@id=\"a-page\"]/main/div/div/div/div/div/div[2]/a')[0].click()\n",
    "            soup = BeautifulSoup(chromedriver.page_source)\n",
    "            new_df['release_month'][i]=to_date(get_movie_value(soup, 'Earliest Release Date'))\n",
    "            new_df['genre'][i]=genre_list(get_movie_value(soup, 'Genres'))\n",
    "            new_df['distributor'][i]=production_company(get_movie_value(soup, 'Domestic Distributor'))\n",
    "            new_df['MPAA_rating'][i]=get_movie_value(soup, 'MPAA')\n",
    "            new_df['domestic_boxoffice'][i]=money_to_int(soup_find_money(soup))\n",
    "            new_df['budget'][i]=money_to_int(get_movie_value(soup, 'Budget'))\n",
    "            new_df['running_time'][i]=runtime_to_minutes(get_movie_value(soup, 'Running Time'))\n",
    "            i+=1\n",
    "        else:\n",
    "            new_df['release_month'][i]=''\n",
    "            new_df['genre'][i]=''\n",
    "            new_df['distributor'][i]=''\n",
    "            new_df['MPAA_rating'][i]=''\n",
    "            new_df['domestic_boxoffice'][i]=''\n",
    "            new_df['budget'][i]=''\n",
    "            new_df['running_time'][i]=''\n",
    "            i+=1\n",
    "    return new_df\n",
    "#uncomment below to run\n",
    "#boxoffice_mojo_scrape_bot(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's only select non-empty rows\n",
    "\n",
    "new_df=new_df.loc[(new_df['domestic_boxoffice']!='')].reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets make a separate dataframe with only the oscar winners\n",
    "oscars_winner=new_df.loc[new_df['oscar_win']!=0]['movie'].reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection part 3: TV tropes\n",
    "\n",
    "A great study done by data scientists over at DW shows that there are certain 'tropes',or common motifs found among oscar winning and nominated movies. A list of the top 200 tropes is provided here:https://dw-data.github.io/movie-tropes/. This section involves creating another selenium web scrape bot that will scrape tv tropes for each of the movies on our list for those 200 tropes. The total number of tropes a movie has that matches those 200 is displayed in the column named 'tropes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, lets scrape the DW website for the top 200 tropes. \n",
    "#If you are using this code you will have to mannually change the table display setting to display all 200 tropes\n",
    "chromedriver.get(\"https://dw-data.github.io/movie-tropes/\")\n",
    "\n",
    "soup = BeautifulSoup(chromedriver.page_source)\n",
    "table=soup.find('table')\n",
    "tropes=[]\n",
    "rows = [row for row in table.find_all('a')]\n",
    "for row in rows:\n",
    "    tropes.append(row.text.split('\\n'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's clean the data a bit by changing the format of some of the strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tropes)):\n",
    "    if tropes[i] == ['\\\\The Reason You Suck\\\\\" Speech\"']:\n",
    "        tropes[i]=['\"The Reason You Suck\" Speech']\n",
    "    elif tropes[i]==['Heel\\x96Face Turn']:\n",
    "        tropes[i]=['Heel-Face Turn']\n",
    "    elif tropes[i]==['Big \\\\NO!\\\\\"\"']:\n",
    "        tropes[i]=['Big \"NO!\"']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the list of top 200 tropes it is time to scrape tvtropes.org.\n",
    "Here is a selenium bot that will help us automate that process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tv tropes data scrape bot\n",
    "def tropes_identifier(new_df):\n",
    "    for i in range(len(new_df)):\n",
    "        chromedriver.get(\"http://google.com\")\n",
    "        inputElems=chromedriver.find_elements_by_css_selector('input[name=q]')\n",
    "        for inputElem in inputElems:\n",
    "            inputElem.send_keys(new_df['movie'][i]+'(film)'+' tv tropes')\n",
    "            inputElem.send_keys(Keys.ENTER)\n",
    "\n",
    "        results=chromedriver.find_elements_by_xpath('//*[@id=\"rso\"]/div[1]/div/div[1]/a/h3')\n",
    "        for x in results:\n",
    "            x.click()\n",
    "        soup = BeautifulSoup(chromedriver.page_source)\n",
    "        tropelist=soup.find_all('a',class_='twikilink')\n",
    "        counter=0\n",
    "        for item in tropelist:\n",
    "            if item.text.split('\\n') in tropes:\n",
    "                counter+=1\n",
    "        new_df['tropes'][i]=counter\n",
    "    return(new_df)\n",
    "# uncomment below to run the function\n",
    "# tropes_identifier(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9540ceb92669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tropes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'oscar_win'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(20,10))\n",
    "x=new_df['tropes']\n",
    "y=new_df['oscar_win']\n",
    "plt.bar(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do a save of our current data!\n",
    "new_df.to_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection part 4: ratings(IMDB,rotten tomatoes)\n",
    "for this section I am going to webscrape google to find the ratings for each movie from the IMDB and rotten tomatoes. IMDB reviews are more user based whereas rotten tomatoes is more critic based.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df['rt_score']=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_rottentomatoes(soup):\n",
    "    try:\n",
    "        rating=(soup.find(\"div\",class_=\"meta-value\").text).split('(')\n",
    "        rating=rating[0].strip()\n",
    "        return rating\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def score_rottentomatoes(soup):\n",
    "    try:\n",
    "        score=soup.find(\"span\",class_=\"mop-ratings-wrap__percentage\").text\n",
    "        score=int(score.strip().replace('%',''))\n",
    "        return score\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def score_from_google_search(soup):\n",
    "    try:\n",
    "        score=int(rating.split('%')[0].split(':')[1].strip())\n",
    "        return score\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotten_tomatoes(new_df):\n",
    "    for i in range(len(new_df['rt_score'])):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(chromedriver.page_source)\n",
    "score=soup.find(\"span\",class_=\"mop-ratings-wrap__percentage\").text\n",
    "score=int(score.strip().replace('%',''))\n",
    "\n",
    "\n",
    "rating=(soup.find(\"div\",class_=\"meta-value\").text).split('(')\n",
    "rating=rating[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find(\"div\",class_=\"meta-value\").text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df=movies_df[['year','movie']]\n",
    "\n",
    "new_df['year']=''\n",
    "for i in range(len(new_df)):\n",
    "    index=join_df.loc[join_df['movie']==new_df['movie'][i]].index\n",
    "    year=str(join_df['year'][index])\n",
    "    replace=(year.split('\\s')[0].split('\\n')[0])\n",
    "    year=re.sub('.*\\s\\s\\s\\s','',replace)\n",
    "    new_df['year'][i]=year\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tv tropes data scrape bot(v1)\n",
    "def rotten_tomatoes_scrapev1(new_df):\n",
    "    for i in range(len(new_df)):\n",
    "        chromedriver.get(\"http://google.com\")\n",
    "        inputElems=chromedriver.find_elements_by_css_selector('input[name=q]')\n",
    "        for inputElem in inputElems:\n",
    "            inputElem.send_keys(new_df['movie'][i]+'(film) '+new_df['year'][i]+' rotten tomatoes')\n",
    "            inputElem.send_keys(Keys.ENTER)\n",
    "\n",
    "        results=chromedriver.find_elements_by_xpath('//*[@id=\"rso\"]/div[1]/div/div[1]/a/h3')\n",
    "        for x in results:\n",
    "            x.click()\n",
    "        soup = BeautifulSoup(chromedriver.page_source)\n",
    "        new_df['rt_score'][i]=score_rottentomatoes(soup)\n",
    "        if new_df['MPAA_rating'][i]==None:\n",
    "            new_df['MPAA_rating'][i]=rating_rottentomatoes(soup)\n",
    "            \n",
    "    return(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The far superior version compared to v1, but might get flagged by google as a bot if running for too long\n",
    "\n",
    "def rotten_tomatoes_scrapev2(new_df):\n",
    "    for i in range(len(new_df)):\n",
    "        if bool(new_df['rt_score'][i])==False:\n",
    "            chromedriver.get(\"http://google.com\")\n",
    "            inputElems=chromedriver.find_elements_by_css_selector('input[name=q]')\n",
    "            for inputElem in inputElems:\n",
    "                inputElem.send_keys(new_df['movie'][i]+'(film) '+new_df['year'][i]+' rotten tomatoes')\n",
    "                inputElem.send_keys(Keys.ENTER)\n",
    "            soup = BeautifulSoup(chromedriver.page_source)\n",
    "            new_df['rt_score'][i]=score_from_google_search(soup)\n",
    "        \n",
    "\n",
    "            \n",
    "    return(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run the function\n",
    "#rotten_tomatoes_scrapev1(new_df)\n",
    "#rotten_tomatoes_scrapev2(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('movie_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver.get(\"http://google.com\")\n",
    "inputElems=chromedriver.find_elements_by_css_selector('input[name=q]')\n",
    "for inputElem in inputElems:\n",
    "    inputElem.send_keys(new_df['movie'][247]+'(film) '+new_df['year'][247]+' rotten tomatoes')\n",
    "    inputElem.send_keys(Keys.ENTER)\n",
    "soup = BeautifulSoup(chromedriver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=soup.find('div',class_=\"dhIWPd f\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(new_df['rt_score'][7])==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection part 5: original story?\n",
    "For this section I am going to see if a movie's source material has anything to do with its oscar chances.\n",
    "For example, movies like 'fight club','12 years a slave', and many more are all based on a novels or other forms\n",
    "of written work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_table(soup):\n",
    "    try:\n",
    "        table=soup.find('table')\n",
    "        rows = [row for row in table.find_all('tr')]\n",
    "        return rows\n",
    "    except:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['book_based']=''\n",
    "def based_on(new_df):\n",
    "    for i in range(len(new_df)):\n",
    "        chromedriver.find_elements_by_xpath('//*[@id=\"searchInput\"]')[0].send_keys(new_df['movie'][i]+'(film) '+new_df['year'][i])\n",
    "        chromedriver.find_elements_by_xpath('//*[@id=\"searchInput\"]')[0].send_keys(Keys.ENTER)\n",
    "        results=chromedriver.find_elements_by_xpath('//*[@id=\"mw-content-text\"]/div[3]/ul/li[1]/div[1]/a/span[1]')\n",
    "        for x in results:\n",
    "            x.click()\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(chromedriver.page_source)\n",
    "        rows=soup.find_all('th')\n",
    "        if bool('Based on' in str(rows)):\n",
    "            new_df['book_based'][i]=1\n",
    "        else:\n",
    "            new_df['book_based'][i]=0\n",
    "    return new_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "based_on(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(chromedriver.page_source)\n",
    "find_table(soup1)\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://en.wikipedia.org/wiki/12_Years_a_Slave_(film)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection part 6: supplement budget and boxoffice info\n",
    "The previously collected data from boxofficemojo seems to be incomplete. Now,I am going to try to supplement the data set by scraping the data found on The Numbers website:https://www.the-numbers.com/movie/budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "chromedriver=webdriver.Chrome(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helps us scrape the whole website for box office and budget data and put them in a dictionary\n",
    "i=1\n",
    "data=[]\n",
    "while i < 6001:\n",
    "    chromedriver.get(\"https://www.the-numbers.com/movie/budgets/all/\"+str(i))\n",
    "    soup = BeautifulSoup(chromedriver.page_source)\n",
    "    table=soup.find('table')\n",
    "    rows=table.find_all('td')\n",
    "\n",
    "    for row in rows:\n",
    "        data.append(row.text)\n",
    "    i+=100\n",
    "\n",
    "the_numbers={}\n",
    "i=2\n",
    "while i <len(data):\n",
    "    the_numbers[data[i].lower()]=[int(data[i+1].split('$')[1].replace('$', '').replace(',', ''))]+[int(data[i+2].split('$')[1].replace('$', '').replace(',', ''))]\n",
    "    i+=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions that help us match the movies from our dataset with the dataset scraped from The Numbers website\n",
    "from fuzzywuzzy import process\n",
    "def movie_budget(the_numbers):\n",
    "    try:\n",
    "        budget=the_numbers[get_matches(new_df['movie'][365],movie_list_names)[0][0]]\n",
    "        return budget\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "\n",
    "\n",
    "def box_office(the_numbers):\n",
    "    try:\n",
    "        boxoffice=the_numbers[get_matches(new_df['movie'][365],movie_list_names)[0][0]]\n",
    "        return boxoffice\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_matches(query,choice,limit=1):\n",
    "    results=process.extract(query,choice,limit=limit)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loop supplements the missing values from our current dataset with budget and boxoffice data from the new dataset\n",
    "\n",
    "for i in range(len(new_df)):\n",
    "    if new_df['budget'][i]==None and get_matches(new_df['movie'][i],movie_list_names)[0][1]>95:\n",
    "        get_matches(new_df['movie'][i],movie_list_names)[0][1]\n",
    "        new_df['budget'][i]=movie_budget(the_numbers)\n",
    "    elif new_df['domestic_boxoffice'][i]==None and get_matches(new_df['movie'][i],movie_list_names)[0][1]>95:\n",
    "        new_df['domestic_boxoffice'][i]=box_office(the_numbers)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Around 300 missing values were filled using the data from the new website\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_movie_df=new_df[['movie','year','oscar_win','times_nominated','domestic_boxoffice','budget','release_month','MPAA_rating','running_time','genre','tropes','rt_score','book_based']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_movie_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(chromedriver.page_source)\n",
    "links=soup.find_all('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_movies=[]\n",
    "for link in links:\n",
    "    list_of_movies.append((link.text.split('\\n')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_movie_df['true_story_based']=''\n",
    "\n",
    "def based_on_true_story(sorted_movie_df):\n",
    "    for i in range(len(sorted_movie_df)):\n",
    "        if get_matches(sorted_movie_df['movie'][i],list_of_movies)[0][1] >95:\n",
    "            sorted_movie_df['true_story_based'][i]=1\n",
    "        else:\n",
    "            sorted_movie_df['true_story_based'][i]=0\n",
    "    return sorted_movie_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "based_on_true_story(sorted_movie_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_movie_df.to_csv('sorted_movie_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_movie_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('sorted_movie_data2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
